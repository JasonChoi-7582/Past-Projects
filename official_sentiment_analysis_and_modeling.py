# -*- coding: utf-8 -*-
"""Official Sentiment Analysis and Modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fixrwDTNBnQIPbzvu2PeFSOQoJrhcji3

# Data Gathering
"""

# !pip install config
# from config import app_id, api_key
! pip install twython 

from bs4 import BeautifulSoup as BS
import requests
import json
import time
import datetime

import pandas as pd
import numpy as np
import datetime

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer

from textblob import TextBlob

import matplotlib.pyplot as plt
import seaborn           as sns
sns.set_style("darkgrid")

"""## Obtaining NY Times Metadata using NYT API """

# function to request 10 articles (1 page of results) containing 'tesla' from NY Times API
# sorted from newest to oldest in a requested year
# returns a dictionary containing web URL, article snippet (headline), publication date and lead paragraph
def nyt_by_year_and_page(year, page, company):  # must enter company as a string
    app_id = '66664cbd-7f65-4052-996f-d00d97a4047a'
    api_key = 'x4t4RrAqaGqH8MRges7tARYV7bGSrBfu'
    # example link: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=tesla&sort=newest&begin_date=20190101&end_date=20201231&page=1&fl=web_url&fl=snippet&fl=pub_date&fl=_id&fl=lead_paragraph&api-key=x4t4RrAqaGqH8MRges7tARYV7bGSrBfu
    # root_url   = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?q=tesla&sort=newest'
    root_url   = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?'
    company_query = 'q='+ company
    sort = '&sort=newest'
    begin_date = '&begin_date={}0101'.format(str(year))
    end_date   = '&end_date={}1231'.format(str(year))
    pagination = '&page={}'.format(str(page))
    doc_params = '&fl=web_url&fl=snippet&fl=pub_date&fl=_id&fl=lead_paragraph'
    
    url        = root_url + company_query + sort + begin_date + end_date + pagination + doc_params + '&api-key=' + api_key
    print(url)
    
    response   = requests.get(url)
    
    return response.json()

# request all pages of articles for a single year
# returns a list of article dictionaries
def nyt_by_year(year, company):  # must enter company as string
    annual_articles = []
    
    # find how many articles (hits) contain in a calendar year
    total_results = nyt_by_year_and_page(year,0,company)
    hits  = total_results['response']['meta']['hits']

    # request all available pages
    # sleep 7 seconds to clear NYT API rate limit
    for i in range(int(hits/10)): # hits/10 because it shows 10 articles per page 
        query = nyt_by_year_and_page(year,i,company)
        if len(query) == 0: 
          continue
        if "errors" in query: 
          continue
        if "status" in query and query["status"] != "OK": 
          print("\nstatus not in query\n")
          continue 
        if "response" not in query: 
          print("\nresponse not found\n")
          continue 
        # gets the metadata for each article and adds it to annual articles list
        annual_articles = annual_articles + query['response']['docs']
        time.sleep(7)
    # return list of metadata of articles from a specific year by page 
    # each item in the list is a dictionary of metadata on articles (10) 
    # each item is one page of results on Tesla articles for that year 
    return annual_articles

# request all pages of articles for all years
# returns a list of article dictionaries
def nyt_by_all_years(years, company): # must enter company as string
    all_articles = []
    
    for year in years:
        annual_articles = nyt_by_year(year, company)
        all_articles = all_articles + annual_articles
        time.sleep(7)
        
    return all_articles

# create a list of dictionaries of all NYT articles containing 'company from 2010 to 2019'
# warning: running this will take over an hour 
# all_years    = list(range(2010,2020))
# all_articles = nyt_by_all_years(all_years,"tesla")

year10 = nyt_by_year(2010,"tesla")

len(year10)

year11 = nyt_by_year(2011,"tesla")

print("Length of year11: ", len(year11))
all_articles = year10 + year11
print("Length of all_articles: ", len(all_articles))

year12 = nyt_by_year(2012,"tesla")

print("Length of year12: ", len(year12))
all_articles = all_articles + year12
print("Length of all_articles: ", len(all_articles))

year13 = nyt_by_year(2013,"tesla")

print("Length of year13: ", len(year13))
all_articles = all_articles + year13
print("Length of all_articles: ", len(all_articles))

year14 = nyt_by_year(2014,"tesla")

print("Length of year14: ", len(year14))
all_articles = all_articles + year14
print("Length of all_articles: ", len(all_articles))

year15 = nyt_by_year(2015,"tesla")

print("Length of year15: ", len(year15))
all_articles = all_articles + year15
print("Length of all_articles: ", len(all_articles))

year16 = nyt_by_year(2016,"tesla")

print("Length of year16: ", len(year16))
all_articles = all_articles + year16
print("Length of all_articles: ", len(all_articles))

year17 = nyt_by_year(2017,"tesla")

print("Length of year17: ", len(year17))
all_articles = all_articles + year17
print("Length of all_articles: ", len(all_articles))

year18 = nyt_by_year(2018,"tesla")

print("Length of year18: ", len(year18))
all_articles = all_articles + year18
print("Length of all_articles: ", len(all_articles))

year19 = nyt_by_year(2019,"tesla")

print("Length of year19: ", len(year19))
all_articles = all_articles + year19
print("Length of all_articles: ", len(all_articles))

year20 = nyt_by_year(2020,"tesla")

print("Length of year20: ", len(year20))
all_articles = all_articles + year20
print("Length of all_articles: ", len(all_articles))

# save and read JSON with raw API results
with open("nyt_tesla_meta.json", "w") as write_file:
    json.dump(all_articles, write_file)
    
with open("nyt_tesla_meta.json", "r") as read_file:
    data = json.load(read_file)

"""## Obtaining NY Times Articles Content"""

# web scrape NYT for all articles containing 'tesla' using web URLs from API
# returns article content as string
def get_nyt_text(url):
    # get web URL HTML
    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}
    page = requests.get(url, headers=headers, timeout=120)
    
    # get article content from HTML
    soup = BS(page.content, 'html.parser')
    content = soup.findAll('p', class_ = 'css-axufdj evys1bk0')
    
    nyt = ''
    for index in range(len(content)):
        nyt += content[index].get_text()
        
    return nyt

len(data)

# get article content from url provided and store it with JSON file 
# this will take a long time to run because of a large number of urls it will parse (runtime = 1hr+)
for i in range(len(data)):
    print(i)
    print(data[i]['web_url'])
    data[i]['article'] = get_nyt_text(data[i]['web_url'])

"""## Natural Language Processing using VADER Sentiment Analysis Package"""

# perform sentiment analysis of each article snippet and lead paragraph using VADER sentiment analysis
si = SentimentIntensityAnalyzer()

for i in range(len(data)):
    data[i]['VADER snippet neg']      = si.polarity_scores(data[i]['snippet'])['neg']
    data[i]['VADER snippet neu']      = si.polarity_scores(data[i]['snippet'])['neu']
    data[i]['VADER snippet pos']      = si.polarity_scores(data[i]['snippet'])['pos']
    data[i]['VADER snippet compound'] = si.polarity_scores(data[i]['snippet'])['compound']
    
    data[i]['VADER lead neg']      = si.polarity_scores(data[i]['lead_paragraph'])['neg']
    data[i]['VADER lead neu']      = si.polarity_scores(data[i]['lead_paragraph'])['neu']
    data[i]['VADER lead pos']      = si.polarity_scores(data[i]['lead_paragraph'])['pos']
    data[i]['VADER lead compound'] = si.polarity_scores(data[i]['lead_paragraph'])['compound']

"""## Natural Language Processing using TextBlob Sentiment Analysis Package"""

# perform sentiment analysis of each article snippet, lead paragraph and article using TextBlob sentiment analysis
from textblob import TextBlob

for i in range(len(data)):
    data[i]['TextBlob snippet polarity']     = TextBlob(data[i]['snippet']).sentiment[0]
    data[i]['TextBlob snippet subjectivity'] = TextBlob(data[i]['snippet']).sentiment[1]
    
    data[i]['TextBlob lead polarity']        = TextBlob(data[i]['lead_paragraph']).sentiment[0]
    data[i]['TextBlob lead subjectivity']    = TextBlob(data[i]['lead_paragraph']).sentiment[1]
    
    data[i]['TextBlob article polarity']     = TextBlob(data[i]['article']).sentiment[0]
    data[i]['TextBlob article subjectivity'] = TextBlob(data[i]['article']).sentiment[1]

# save JSON with VADER and TextBlob natural language processing results
with open("nyt_tesla_articles_vader_textblob.json", "w") as write_file:
    json.dump(data, write_file)

# This is a test: code to be taken out later
with open("nyt_tesla_articles_vader_textblob.json", "r") as reading_file:
    data = json.load(reading_file)

# This is a test: code to be taken out later
data

"""## Cleaning and Organizing DataFrame Entries by Date"""

# count all web URLs where web scraping could not retrieve article content
null_articles = 0
full_articles = 0

for i in range(len(data)):
    
    if len(data[i]['article']) == 0:
        null_articles += 1
    else:
        full_articles += 1

# number of empty articles
null_articles

# number of retrieved articles
full_articles

# create DataFrame with of NLP sentiment analysis
sentiment = pd.DataFrame(data)
# found the date the article was published, convert it to datetime object, take out the time part of the pub_date
sentiment['date'] = pd.to_datetime(sentiment.pub_date).dt.date
sentiment.head() # prints the first 5 entries

# remove records where article content could not be retrieved from web URLs
sentiment_clean = sentiment[sentiment['TextBlob article subjectivity'] != 0]
sentiment_clean#.head()

# group articles by date
sentiment_clean_less = sentiment_clean.groupby('date').mean()
sentiment_clean_less

article_count_data = sentiment_clean[['date','_id']]
article_count_data = article_count_data.groupby('date').agg(article_count=pd.NamedAgg(column='date', aggfunc='count'))
article_count_data

# merge DataFrames
sentiment_counts = sentiment_clean_less.merge(article_count_data, left_on='date', right_index=True)
sentiment_counts

# pickle and save final DataFrame
sentiment_counts.to_pickle("sentiments_article_counts.pkl")

"""## Creating DataFrame with Sentiment Analysis for each Date"""

# review final DataFrame
sentiment_clean = pd.read_pickle("sentiments_article_counts.pkl")
sentiment_clean.head()

# plot statistical distributions of article sentiment
import matplotlib.pyplot as plt
axes = sentiment_clean.hist(figsize=(14,14))

"""## Comparing VADER Sentiment Analysis with Stock Trends"""

import json

with open('nyt_tesla_meta.json') as f:
    d = json.load(f)
    print(d)

sentiment = pd.read_pickle("sentiments_article_counts.pkl")

sentiment.index = pd.DatetimeIndex(sentiment.index)
sentiment

# created Tesla dataframe from csv file about Tesla stocks 
TSLA_DF = pd.read_csv('TSLA.csv')
# calculated the first difference by subtracting the current row closing price from the closing price of the previous row
TSLA_DF['First_Diff'] = TSLA_DF.Close - TSLA_DF.Close.shift(1)
TSLA_DF['Date'] = pd.to_datetime(TSLA_DF.Date)

# merge Tesla sentiment analysis dataframe with Tesla stock trends dataframe
TSLA_sentiment = sentiment.merge(TSLA_DF, left_index=True, right_on='Date')

# create new column 'Movement' in Tesla dataframe 
TSLA_sentiment['Movement'] = np.where(TSLA_sentiment['First_Diff'] > 0, 1, 0)

# cleaning Tesla dataframe by removing sentiments with a more neutral compound score 
TSLA_sentiment_Cleaned = TSLA_sentiment[TSLA_sentiment['VADER snippet compound'] != 0]
TSLA_sentiment_Cleaned.set_index('Date', inplace=True)
TSLA_sentiment_Cleaned

# creating a NASDAQ dataframe using NASDAQ data from 2010-2019
# adding NASDAQ Movement column according to First difference calculations
Nasdaq = pd.read_csv('Nasdaq2010-2019.csv')
Nasdaq['Nasdaq_First_Diff'] = Nasdaq.Close - Nasdaq.Close.shift(1)
Nasdaq['Nasdaq_Movement'] = np.where(Nasdaq['Nasdaq_First_Diff'] > 0, 1, 0)
Nasdaq['Date'] = pd.to_datetime(Nasdaq.Date)
Nasdaq

# merging the Tesla sentiment and Tesla Stock Trends dataframe with the NASDAQ Stock Trends dataframe
Combined = TSLA_sentiment_Cleaned.merge(Nasdaq, left_index=True, right_on='Date')
Combined

# cleaning combined dataframe by removing sentiments with a more neutral compound score 
Cleaned = Combined[Combined['VADER snippet compound'] != 0]
Cleaned.set_index('Date', inplace=True)
Cleaned

Cleaned.columns

"""### Baseline Log Model with VADER """

from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import PolynomialFeatures

from itertools import combinations
import warnings
warnings.filterwarnings('ignore')

X = Cleaned[['VADER snippet compound', 'VADER snippet neg', 'VADER snippet neu', 'VADER snippet pos', 'article_count', 'Volume_x', 'Nasdaq_Movement']]
y = Cleaned['Movement']

columns = X.columns

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=35)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

scaler = StandardScaler()
scaler.fit(X_train)
X_train =pd.DataFrame(data=scaler.transform(X_train), columns=columns)
X_test =pd.DataFrame(data=scaler.transform(X_test), columns=columns)

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(fit_intercept = False, C = 1e12)
logreg.fit(X_train, y_train)
dictionary = dict(zip(list(X_train.columns), list(logreg.coef_[0])))
dictionary

y_pred_class_BLV = logreg.predict(X_test)

from sklearn import metrics
print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_class_BLV))
print('Test F1 score: ', metrics.f1_score(y_test, y_pred_class_BLV))

from sklearn.metrics import confusion_matrix

cmBV = confusion_matrix(y_test, y_pred_class_BLV)
classes = ['Moved Up', "Moved Down"]

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion Matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="black" if cm[i, j] > thresh else "white")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    
import itertools
plot_confusion_matrix(cmBV, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Pastel1_r
                    )

"""### Naive Bayes Model with VADER"""

from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_train, y_train)

y_pred_NBV = clf.predict(X_test)
y_pred_NBV.shape

clf.predict_proba(X_test)

clf.score(X_test, y_test)

from sklearn.metrics import confusion_matrix

cmNBV = confusion_matrix(y_test, y_pred_NBV)
classes = ['Moved Up', "Moved Down"]

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion Matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="black" if cm[i, j] > thresh else "purple")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    
import itertools
plot_confusion_matrix(cmNBV, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Pastel1_r)